{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Testing ANN under different FS approaches\n",
        "Testing the best estimator of ANN and RS for testing potato yield by Control treatment using different approaches of feature selection. Three feature selections were considered such as Univariate, Chi2, and Permutation Importance. The best accuracy was achieved with Permutation importance (R2=0.78), followed by Univariate (R2=0.72), and Chi2 (R2=0.65) when tested potato yield with Control treatment in Rwanda. \n",
        "This contribution is mapped as a contribution from the Egypt Use Case team for the TRANSFORM WP under EiA"
      ],
      "metadata": {
        "id": "qkAI-oFkVvtr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset source: Potato yield in Rowanda under soil, topgraphy and climate datasetcreating 69 features."
      ],
      "metadata": {
        "id": "jLVSEFX3YXjZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we will train and test the best models of RF and ANN using all features and without applying feature selection approaches to explore the primary accuracy. Then, we will apply three feature selection approaches to the best estimator (ANN) to explore the best FS approach."
      ],
      "metadata": {
        "id": "BlR6vhS9aRZd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uEpggF7Vf6Q"
      },
      "outputs": [],
      "source": [
        "#Import the required libraries\n",
        "import os   #to interact with the underlying operating system\n",
        "import random   #for generating random integers in Python\n",
        "import warnings   #alerts of unexpected conditions detected when running a code\n",
        "import numpy as np  #to provide an array object that is up to 50x faster than traditional Python lists\n",
        "import pandas as pd  #for working with data sets, as it has functions for analyzing, cleaning, exploring, and manipulating data\n",
        "from scipy import stats   #SciPy is a collection of mathematical algorithms and convenience functions built on the NumPy extension of Python\n",
        "import matplotlib.pyplot as plt #a cross-platform, data visualization and graphical plotting library (histograms, scatter plots, bar charts, etc) for Python and its numerical extension NumPy\n",
        "from sklearn import ensemble,neural_network,neighbors,svm,model_selection,inspection #neighbors,svm if we want to use Knearest and SVM models\n",
        "warnings.simplefilter(action='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the dataset (here we used Rowanda Potato dataset)\n",
        "df = pd.read_csv('D:/ICARDA/TT2/TestingByControl_1/Final merged file/Cleaned2.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "ULOEbpLNdV27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define X and y features"
      ],
      "metadata": {
        "id": "ha9hVfuQdfUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First use all featues to train and validate the models (ANN and RF),in order to detect the important features and best estimator\n",
        "xFeatures = ['B030','Cu030','K030','N030','Na030','P030','Ptot030','altop','albottom','bdr','ctottop','ctotbottom','catop','cabottom',\n",
        "            'claytotpsatop','claytotpsabottom','dbodtop','dbodbottom','ececftop','ececfbottom','fetop','febottom','ktop','kbottom','mgtop',\n",
        "            'mgbottom','ntotncstop','ntotncsbottom','octop','ocbottom','ptop','pbottom','phh2otop','phh2obottom','stop','sbottom','sandtotpsatop',\n",
        "            'sandtotpsabottom','silttotpsatop','silttotpsabottom','wpg2top','wpg2bottom','zntop','znbottom','SOMtop','SOMbottom','PWPtop',\n",
        "            'PWPbottom','FCtop','FCbottom','SWStop','SWSbottom','treat2','DEM','slope','TPI','TRI','tr','di','nrd','tmean','tmin','tmax','Nrate','Prate','Krate']\n",
        "#Identify the dependent variable\n",
        "yFeatur = 'yield'"
      ],
      "metadata": {
        "id": "EAMJFBs0diUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the work space"
      ],
      "metadata": {
        "id": "ooF5Z4__doY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Specify the workspace INfile and outfile for training\n",
        "inFile = 'D:/ICARDA/TT2/TestingByControl_1/Final merged file'\n",
        "outFolder = 'D:/ICARDA/TT2/TestingByControl_1/Final merged file/Kheir_Codes and results for building ML to predict potato in Rwanda/ANN_ICARDA_Training'"
      ],
      "metadata": {
        "id": "L9zWW-F1dqg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data pre-processing:Remove minus and NAN values"
      ],
      "metadata": {
        "id": "TXK-GkpJmr1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.replace([np.inf, -np.inf], np.nan)\n",
        "df = df.dropna()\n",
        "df = df.reset_index()"
      ],
      "metadata": {
        "id": "GqFfgGn4m5ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Required functions for the statistics, visualization and hyperparameter tuning"
      ],
      "metadata": {
        "id": "RG6BwEA8dt9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Statistics, visualization and hyperparameter functions\n",
        "def performanceStatistics(x, y):  #Statistics function\n",
        "  E = y - x                       # Error\n",
        "  AE = np.abs(E)                  # Absolute Error\n",
        "  MAE = np.mean(AE)               # Mean Absolute Error\n",
        "  SE = np.power(E, 2)             # Square Error\n",
        "  MSE = np.mean(SE)               # Mean Square Error (this method is the best)\n",
        "  RMSE = np.sqrt(MSE)             # Root Mean Square Error\n",
        "  RB = ((np.mean(y) - np.mean(x)) / np.mean(x)) * 100\n",
        "  slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
        "  R2 = np.power(r_value, 2)       # correlation of determination\n",
        "  stat = {'R2':round(R2, 2),\n",
        "          'RB':round(RB, 2),\n",
        "          'MAE':round(MAE, 2),\n",
        "          'RMSE':round(RMSE, 2),\n",
        "          'n':len(E)}\n",
        "  return stat\n",
        "\n",
        "def plotOriginalPredicted(Original, Predicted, outFile, label=''): #visualization function\n",
        "  minX = np.min(Original)\n",
        "  maxX = np.max(Original)\n",
        "  minY = np.min(Predicted)\n",
        "  maxY = np.max(Predicted)\n",
        "  minXY = np.min(np.array([minX, minY]))\n",
        "  maxXY = np.min(np.array([maxX, maxY]))\n",
        "  fs = 26\n",
        "  fig = plt.figure(figsize=(15, 15))\n",
        "  plt.scatter(Original, Predicted, color='blue', label=label)\n",
        "  plt.plot(np.linspace(minXY, maxXY, 50), np.linspace(minXY, maxXY, 50), color='red', linestyle='-', linewidth=1, markersize=5, label='1:1 Line')\n",
        "  plt.xlim([minXY, maxXY])\n",
        "  plt.ylim([minXY, maxXY])\n",
        "  plt.xticks(size = fs)\n",
        "  plt.yticks(size = fs)\n",
        "  plt.xlabel('Original yield (t/ha)', fontsize=fs)\n",
        "  plt.ylabel('Predicted yield (t/ha)', fontsize=fs)\n",
        "  \n",
        "  stat = performanceStatistics(Original, Predicted)\n",
        "  digits = 2\n",
        "  n = round(stat['n'], digits)\n",
        "  r2 = round(stat['R2'], digits)\n",
        "  RB = round(stat['RB'], digits)\n",
        "  MAE = round(stat['MAE'], digits)\n",
        "  RMSE = round(stat['RMSE'], digits)\n",
        "  s = 'n={} \\n$R^2$={}\\nRB={} (%)\\nMAE={} (t/ha)\\nRMSE={} (t/ha)'.format(n, r2, RB, MAE, RMSE)\n",
        "  plt.text(x=(minXY + 1), y=(maxXY - 1), s=s, horizontalalignment='left', verticalalignment='top', color='black', fontsize=fs)\n",
        "  plt.legend(loc= 9, fontsize=fs)\n",
        "  plt.savefig(outFile, dpi=300, bbox_inches='tight')\n",
        "  plt.clf()\n",
        "  plt.close()\n",
        "\n",
        "def hyperparameters(n):    #Hyperparameters function using the grid search\n",
        "  \n",
        "  RFR_param_grid = dict(n_estimators = [i for i in range(100, 2050, 50)], #RF hyperparameter tuning\n",
        "                        max_features = ['auto', 'sqrt', 'log2'],\n",
        "                        )\n",
        "\n",
        "  hl = [i for i in range(10, 55, 5)] # hidden layers for ANN huperparameter tuning\n",
        "  hl = [25] \n",
        "  hn = [(n * 2) + 1] # hidden neurons\n",
        "  hlhn = []\n",
        "  for i in hl:\n",
        "    for j in hn:\n",
        "      hlhn.append((i, j))\n",
        "\n",
        "  hls = [(i[0], i[1]) for i in hlhn]\n",
        "  ANN_param_grid = dict(hidden_layer_sizes = hls , \n",
        "                        activation = ['identity', 'logistic', 'tanh', 'relu'], # 'identity', 'logistic', 'tanh', 'relu'\n",
        "                        solver = ['lbfgs', 'sgd', 'adam'], # 'lbfgs', 'sgd', 'adam'\n",
        "                        max_iter = [1000000],\n",
        "                        )\n",
        "  \n",
        "  #KNN_param_grid = dict(n_neighbors = [i for i in range(10, 55, 5)],\n",
        "                        #)\n",
        "   \n",
        "  #SVR_param_grid = dict(gamma = [0.0625,0.25,0.5,1,2,4],\n",
        "                        #C = [10, 20, 30, 40],\n",
        "                        #)\n",
        "  \n",
        "  MLA = {\n",
        "      # https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
        "      'RFR':[ensemble.RandomForestRegressor(random_state=0, verbose=False), RFR_param_grid],\n",
        "      # https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html\n",
        "      'ANN':[neural_network.MLPRegressor(random_state=0, verbose=False), ANN_param_grid],\n",
        "      # https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html\n",
        "      #'KNN':[neighbors.KNeighborsRegressor(n_jobs=-1), KNN_param_grid],\n",
        "      # https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html\n",
        "      #'SVR':[svm.SVR(verbose=0), SVR_param_grid],\n",
        "      }\n",
        "\n",
        "  return MLA"
      ],
      "metadata": {
        "id": "VrHj9XSId2oT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and testing the models using all features and without feature selection"
      ],
      "metadata": {
        "id": "epGiEhRv8lNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Training and validating the models by using 75 % of the datset for training and 25 % for validation\n",
        "df = pd.read_csv('D:/ICARDA/TT2/TestingByControl_1/Final merged file/Cleaned2.csv')\n",
        "Xdf = df[xFeatures]\n",
        "ydf = df[[yFeatur]]\n",
        "\n",
        "X = df[xFeatures]\n",
        "y = df[[yFeatur]]\n",
        "X = (X - Xdf.mean()) / Xdf.std()\n",
        "y = (y - ydf.mean()) / ydf.std()\n",
        "df = pd.concat([X, y], axis = 1)\n",
        "df = df.dropna()\n",
        "X = df[xFeatures]\n",
        "y = df[[yFeatur]]\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25, random_state=0)\n",
        "\n",
        "n = len(xFeatures)\n",
        "MLA = hyperparameters(n)\n",
        "for idMLA, mlaL_mlaA in enumerate(MLA.items()):\n",
        "  mlaL, mlaA = mlaL_mlaA\n",
        "  estimator = mlaA[0]\n",
        "  param_grid = mlaA[1]\n",
        "  GSCV = model_selection.GridSearchCV(estimator, param_grid, scoring='r2', cv=5, refit=True, verbose=False, n_jobs=-1)\n",
        "  GSCV.fit(X_train, y_train)\n",
        "  bestEstimator = GSCV.best_estimator_ \n",
        "\n",
        "  OriginalYield_test  = y_test.values.flatten()\n",
        "  PredictedYield_test = bestEstimator.predict(X_test).flatten()\n",
        "  OriginalYield_test  = (float(ydf.std()) * OriginalYield_test) + float(ydf.mean())\n",
        "  PredictedYield_test = (float(ydf.std()) * PredictedYield_test) + float(ydf.mean())\n",
        "  \n",
        "  outFile     = os.path.join(outFolder, '{}_plot_DefaultML_{}.png'.format(mlaL, n))\n",
        "  Stats       = os.path.join(outFolder, '{}_Stats_DefaultML_{}.xlsx'.format(mlaL, n))\n",
        "  Importances = os.path.join(outFolder, '{}_Importances_DefaultML_{}.xlsx'.format(mlaL, n))\n",
        "\n",
        "  plotOriginalPredicted(OriginalYield_test, PredictedYield_test, outFile, label='Predicted yield of {}'.format(mlaL))\n",
        "  df_stats = performanceStatistics(OriginalYield_test, PredictedYield_test)\n",
        "  print(mlaL, df_stats)\n",
        "\n",
        "  df_GSCV = pd.DataFrame(GSCV.cv_results_)\n",
        "  df_GSCV = df_GSCV.sort_values(by='rank_test_score', ascending=True)\n",
        "  df_GSCV = df_GSCV.head(1)\n",
        "  for i in list(df_GSCV.columns):\n",
        "    df_stats[i] = df_GSCV[i].values.tolist()[0]\n",
        "  df_stats = pd.DataFrame.from_dict(df_stats, orient='index').T\n",
        "  df_stats.to_excel(Stats, index=False)\n",
        "  \n",
        "  pi = inspection.permutation_importance(bestEstimator, X_train, y_train, n_jobs=-1, random_state=0).importances_mean #to identify the important features based on permutation\n",
        "  pi = [((i / pi.sum()) * 100) for i in pi]\n",
        "  dfImportances = pd.DataFrame(data=[pi], columns=xFeatures).round(2)\n",
        "  dfImportances.to_excel(Importances, index=False)\n"
      ],
      "metadata": {
        "id": "cpHf8lTk8zEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the best estimator by Control Treatment using all features without applying feature selection "
      ],
      "metadata": {
        "id": "eGaWmw9X9L21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing the best estimator by Control Treatment using all features \n",
        "##OutFolder2 where testing ouputs will be placed\n",
        "outFolder = 'D:/ICARDA/TT2/TestingByControl_1/Final merged file/Kheir_Codes and results for building ML to predict potato in Rwanda/ANN_ICARDA_Testing'\n",
        "#Test by b Control \n",
        "for i in [[xFeaturesML, 'ML'], [xFeaturesML, 'ML']]:\n",
        "\n",
        "  df = pd.read_csv('D:/ICARDA/TT2/TestingByControl_1/Final merged file/Cleaned2.csv')\n",
        "  xFeatures = i[0]\n",
        "  group     = i[1]\n",
        "  \n",
        "  print(group)\n",
        "  Xdf = df[xFeatures]\n",
        "  ydf = df[[yFeatur]]\n",
        "\n",
        "  for col in df.columns:\n",
        "    if col not in ['texture_class_top','texture_class_bottom','treat']:\n",
        "      mean = np.mean(df[col].values)\n",
        "      std  = np.std(df[col].values)\n",
        "      if std != 0:\n",
        "        df[col] = df[col].apply(lambda x:(x-mean)/std)\n",
        "  \n",
        "  testtreat = ['Control'] # random.sample(list(df.treat.unique()), 1)\n",
        "  print(testtreat)\n",
        "\n",
        " \n",
        "  test  = df[df['treat'].isin(testtreat)]\n",
        "\n",
        " \n",
        "  X_test  = test[xFeatures]\n",
        "  y_test  = test[[yFeatur]]\n",
        "#Test by treat(Control) and best estimator\n",
        "n = len(xFeatures)\n",
        "  #MLA = hyperparameters(n)\n",
        "  #for idMLA, mlaL_mlaA in enumerate(MLA.items()):\n",
        "    #mlaL, mlaA = mlaL_mlaA\n",
        "    #estimator = mlaA[0]\n",
        "    #param_grid = mlaA[1]\n",
        "    #GSCV = model_selection.GridSearchCV(estimator, param_grid, scoring='r2', cv=5, refit=True, verbose=False, n_jobs=-1)\n",
        "GSCV.predict(X_test)\n",
        "bestEstimator = GSCV.best_estimator_ \n",
        "\n",
        "OriginalYield_test  = y_test.values.flatten()\n",
        "PredictedYield_test = bestEstimator.predict(X_test).flatten()\n",
        "OriginalYield_test  = (float(ydf.std()) * OriginalYield_test) + float(ydf.mean())\n",
        "PredictedYield_test = (float(ydf.std()) * PredictedYield_test) + float(ydf.mean())\n",
        "\n",
        "yieldFile   = os.path.join(outFolder, '{}_yield_{}_{}.xlsx'.format(mlaL, group, n))\n",
        "plot        = os.path.join(outFolder, '{}_plot_{}_{}.png'.format(mlaL, group, n))\n",
        "Stats       = os.path.join(outFolder, '{}_Stats_{}_{}.xlsx'.format(mlaL, group, n))\n",
        "Importances = os.path.join(outFolder, '{}_Importances_{}_{}.xlsx'.format(mlaL, group, n))\n",
        "\n",
        "dfYield = pd.DataFrame({'Original Yield':list(OriginalYield_test), 'Predicted Yield':list(PredictedYield_test)})\n",
        "dfYield.to_excel(yieldFile, index=False)\n",
        "\n",
        "plotOriginalPredicted(OriginalYield_test, PredictedYield_test, plot, label='Predicted yield of {}'.format(mlaL))\n",
        "df_stats = performanceStatistics(OriginalYield_test, PredictedYield_test)\n",
        "print(mlaL, df_stats)\n",
        "\n",
        "df_GSCV = pd.DataFrame(GSCV.cv_results_)\n",
        "df_GSCV = df_GSCV.sort_values(by='rank_test_score', ascending=True)\n",
        "df_GSCV = df_GSCV.head(1)\n",
        "for i in list(df_GSCV.columns):\n",
        "       \n",
        "    df_stats[i] = df_GSCV[i].values.tolist()[0]\n",
        "            \n",
        "    df_stats = pd.DataFrame.from_dict(df_stats, orient='index').T\n",
        "    df_stats.to_excel(Stats, index=False)\n",
        "    \n",
        "    pi = inspection.permutation_importance(bestEstimator, X_test, y_test, n_jobs=-1, random_state=0).importances_mean\n",
        "    pi = [((i / pi.sum()) * 100) for i in pi]\n",
        "    dfImportances = pd.DataFrame(data=[pi], columns=xFeatures).round(2)\n",
        "    dfImportances.to_excel(Importances, index=False)"
      ],
      "metadata": {
        "id": "w8Fh3Wkj9ICO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We found that the best estimator is ANN, and the accuracy is 'R2': 0.52  using all features and without feature selection.\n",
        "- Lets try **different feature selection approaches** such as ***Univariate*** and ***Chi2*** alongwith ***permutation importance*** from ANN to explore the best approach acheived the highest accuracy\n",
        "1-Univariate approach\n",
        "#Statistical tests can be used to select those features that have the strongest relationship with the output variable."
      ],
      "metadata": {
        "id": "QD7ULVUK9jb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The scikit-learn library provides the SelectKBest class that can be used with a suite of different statistical tests to select a specific number of features.\n",
        "\n",
        "#The example below uses the chi-squared (chiÂ²) statistical test for non-negative features to select k (k=10) of the best features from the Mobile Price Range Prediction Dataset.\n",
        "#Load the required libraries \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "df = pd.read_csv(\"D:/ICARDA/TT2/TestingByControl_1/Final merged file/Kheir_Codes and results for building ML to predict potato in Rwanda/Feature selection techniques_basedRFE/Cleaned1.csv\")\n",
        "df"
      ],
      "metadata": {
        "id": "w0ykoPHL9ydu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Preprocessing to get ride of NAN and minus values"
      ],
      "metadata": {
        "id": "GP2arG6w-W-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.apply (pd.to_numeric, errors='coerce')\n",
        "\n",
        "print (df)\n",
        "df[df < 0] = 0\n",
        "df\n",
        "df.isnull().values.any()\n",
        "df = df.dropna()\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "print (df)"
      ],
      "metadata": {
        "id": "B705oXHg-G8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Specify X and y to explore the FS based univariate approach\n",
        "X = df2.iloc[:,1:70]  #independent variables\n",
        "y = df2.iloc[:,-1]    #target variable i.e price range\n",
        "y=y.astype('int') #Convert float to int\n",
        "y"
      ],
      "metadata": {
        "id": "7TCy5Lpk-r1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#apply SelectKBest class to extract top 20 best features\n",
        "\n",
        "BestFeatures = SelectKBest(score_func=chi2, k=20)\n",
        "fit = BestFeatures.fit(X,y)\n",
        "df_scores = pd.DataFrame(fit.scores_)\n",
        "df_columns = pd.DataFrame(X.columns)\n",
        "f_Scores = pd.concat([df_columns,df_scores],axis=1)               # feature scores\n",
        "f_Scores.columns = ['Specs','Score']  \n",
        "f_Scores \n",
        "print(f_Scores.nlargest(20,'Score'))       # print 20 best features in descending order"
      ],
      "metadata": {
        "id": "o9HOYwGP-5iG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test ANN by these 20 features explored by Univariate approach and compare the accuracy with previous one\n",
        "#Univariate Selected features\n",
        "xFeatures = ['ntotncstop','ntotncsbottom','catop','cabottom','Ptot030','P030','K030','Nrate','N030','mgbottom','ktop','kbottom','Krate','Prate','fetop','DEM','Cu030','albottom','TRI']\n",
        "xFeaturesML = ['ntotncstop','ntotncsbottom','catop','cabottom','Ptot030','P030','K030','Nrate','N030','mgbottom','ktop','kbottom','Krate','Prate','fetop','DEM','Cu030','albottom','TRI']\n",
        "yFeatur = 'yield'"
      ],
      "metadata": {
        "id": "TCjGFLfi_TFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training and validating the model using the new features extracted by univariate approach"
      ],
      "metadata": {
        "id": "4r_-PkIa_qZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Training and validating the model using the new features  \n",
        "df = pd.read_csv('D:/ICARDA/TT2/TestingByControl_1/Final merged file/Kheir_Codes and results for building ML to predict potato in Rwanda/Feature selection techniques_basedRFE/Cleaned3.csv')\n",
        "\n",
        "Xdf = df[xFeatures]\n",
        "ydf = df[[yFeatur]]\n",
        "\n",
        "X = df[xFeatures]\n",
        "y = df[[yFeatur]]\n",
        "X = (X - Xdf.mean()) / Xdf.std()\n",
        "y = (y - ydf.mean()) / ydf.std()\n",
        "df = pd.concat([X, y], axis = 1)\n",
        "df = df.dropna()\n",
        "X = df[xFeatures]\n",
        "y = df[[yFeatur]]\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25, random_state=0)\n",
        "\n",
        "n = len(xFeatures)\n",
        "MLA = hyperparameters(n)\n",
        "for idMLA, mlaL_mlaA in enumerate(MLA.items()):\n",
        "  mlaL, mlaA = mlaL_mlaA\n",
        "  estimator = mlaA[0]\n",
        "  param_grid = mlaA[1]\n",
        "  GSCV = model_selection.GridSearchCV(estimator, param_grid, scoring='r2', cv=5, refit=True, verbose=False, n_jobs=-1)\n",
        "  GSCV.fit(X_train, y_train)\n",
        "  bestEstimator = GSCV.best_estimator_ \n",
        "\n",
        "  OriginalYield_test  = y_test.values.flatten()\n",
        "  PredictedYield_test = bestEstimator.predict(X_test).flatten()\n",
        "  OriginalYield_test  = (float(ydf.std()) * OriginalYield_test) + float(ydf.mean())\n",
        "  PredictedYield_test = (float(ydf.std()) * PredictedYield_test) + float(ydf.mean())\n",
        "  \n",
        "  outFile     = os.path.join(outFolder, '{}_plot_DefaultML_{}.png'.format(mlaL, n))\n",
        "  Stats       = os.path.join(outFolder, '{}_Stats_DefaultML_{}.xlsx'.format(mlaL, n))\n",
        "  Importances = os.path.join(outFolder, '{}_Importances_DefaultML_{}.xlsx'.format(mlaL, n))\n",
        "\n",
        "  plotOriginalPredicted(OriginalYield_test, PredictedYield_test, outFile, label='Predicted yield of {}'.format(mlaL))\n",
        "  df_stats = performanceStatistics(OriginalYield_test, PredictedYield_test)\n",
        "  print(mlaL, df_stats)\n",
        "\n",
        "  df_GSCV = pd.DataFrame(GSCV.cv_results_)\n",
        "  df_GSCV = df_GSCV.sort_values(by='rank_test_score', ascending=True)\n",
        "  df_GSCV = df_GSCV.head(1)\n",
        "  for i in list(df_GSCV.columns):\n",
        "    df_stats[i] = df_GSCV[i].values.tolist()[0]\n",
        "  df_stats = pd.DataFrame.from_dict(df_stats, orient='index').T\n",
        "  df_stats.to_excel(Stats, index=False)\n",
        "  \n",
        "  pi = inspection.permutation_importance(bestEstimator, X_train, y_train, n_jobs=-1, random_state=0).importances_mean\n",
        "  pi = [((i / pi.sum()) * 100) for i in pi]\n",
        "  dfImportances = pd.DataFrame(data=[pi], columns=xFeatures).round(2)\n",
        "  dfImportances.to_excel(Importances, index=False)"
      ],
      "metadata": {
        "id": "vVwhH3sT_e7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('D:/ICARDA/TT2/TestingByControl_1/Final merged file/Kheir_Codes and results for building ML to predict potato in Rwanda/Feature selection techniques_basedRFE/Cleaned3.csv')\n",
        "df = df.dropna()\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "print (df)"
      ],
      "metadata": {
        "id": "bdHag_12_w3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test by control treatment after using the important features extracted by univariate approach"
      ],
      "metadata": {
        "id": "GedxvWWq_7Dq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Test by b Control \n",
        "for i in [[xFeaturesML, 'ML'], [xFeaturesML, 'ML']]:\n",
        "\n",
        "  #df = pd.read_csv('D:/ICARDA/TT2/TestingByControl_1/Final merged file/Kheir_Codes and results for building ML to predict potato in Rwanda/Feature selection techniques_basedRFE/Cleaned3.csv')\n",
        "  xFeatures = i[0]\n",
        "  group     = i[1]\n",
        "  \n",
        "  print(group)\n",
        "  Xdf = df[xFeatures]\n",
        "  ydf = df[[yFeatur]]\n",
        "\n",
        "  for col in df.columns:\n",
        "    if col not in ['texture_class_top','texture_class_bottom','treat']:\n",
        "      mean = np.mean(df[col].values)\n",
        "      std  = np.std(df[col].values)\n",
        "      if std != 0:\n",
        "        df[col] = df[col].apply(lambda x:(x-mean)/std)\n",
        "  \n",
        "  testtreat = ['Control'] # random.sample(list(df.treat.unique()), 1)\n",
        "  print(testtreat)\n",
        "\n",
        " \n",
        "  test  = df[df['treat'].isin(testtreat)]\n",
        "\n",
        " \n",
        "  X_test  = test[xFeatures]\n",
        "  y_test  = test[[yFeatur]]\n",
        "###OutFolder2 for test by Control\n",
        "outFolder = 'D:/ICARDA/TT2/TestingByControl_1/Final merged file/Kheir_Codes and results for building ML to predict potato in Rwanda/UnivariateFS_Test'\n",
        "#Test by b Control \n",
        "for i in [[xFeaturesML, 'ML'], [xFeaturesML, 'ML']]:\n",
        "\n",
        "  #df = pd.read_csv('D:/ICARDA/TT2/TestingByControl_1/Final merged file/Kheir_Codes and results for building ML to predict potato in Rwanda/Feature selection techniques_basedRFE/Cleaned3.csv')\n",
        "\n",
        "  xFeatures = i[0]\n",
        "  group     = i[1]\n",
        "  \n",
        "  print(group)\n",
        "  Xdf = df[xFeatures]\n",
        "  ydf = df[[yFeatur]]\n",
        "\n",
        "  for col in df.columns:\n",
        "    if col not in ['texture_class_top','texture_class_bottom','treat']:\n",
        "      mean = np.mean(df[col].values)\n",
        "      std  = np.std(df[col].values)\n",
        "      if std != 0:\n",
        "        df[col] = df[col].apply(lambda x:(x-mean)/std)\n",
        "  \n",
        "  testtreat = ['Control'] # random.sample(list(df.treat.unique()), 1)\n",
        "  print(testtreat)\n",
        "\n",
        " \n",
        "  test  = df[df['treat'].isin(testtreat)]\n",
        "\n",
        " \n",
        "  X_test  = test[xFeatures]\n",
        "  y_test  = test[[yFeatur]]\n",
        "#Test by treat(Control) and best estimator\n",
        "n = len(xFeatures)\n",
        "  #MLA = hyperparameters(n)\n",
        "  #for idMLA, mlaL_mlaA in enumerate(MLA.items()):\n",
        "    #mlaL, mlaA = mlaL_mlaA\n",
        "    #estimator = mlaA[0]\n",
        "    #param_grid = mlaA[1]\n",
        "    #GSCV = model_selection.GridSearchCV(estimator, param_grid, scoring='r2', cv=5, refit=True, verbose=False, n_jobs=-1)\n",
        "GSCV.predict(X_test)\n",
        "bestEstimator = GSCV.best_estimator_ \n",
        "\n",
        "OriginalYield_test  = y_test.values.flatten()\n",
        "PredictedYield_test = bestEstimator.predict(X_test).flatten()\n",
        "OriginalYield_test  = (float(ydf.std()) * OriginalYield_test) + float(ydf.mean())\n",
        "PredictedYield_test = (float(ydf.std()) * PredictedYield_test) + float(ydf.mean())\n",
        "\n",
        "yieldFile   = os.path.join(outFolder, '{}_yield_{}_{}.xlsx'.format(mlaL, group, n))\n",
        "plot        = os.path.join(outFolder, '{}_plot_{}_{}.png'.format(mlaL, group, n))\n",
        "Stats       = os.path.join(outFolder, '{}_Stats_{}_{}.xlsx'.format(mlaL, group, n))\n",
        "Importances = os.path.join(outFolder, '{}_Importances_{}_{}.xlsx'.format(mlaL, group, n))\n",
        "\n",
        "dfYield = pd.DataFrame({'Original Yield':list(OriginalYield_test), 'Predicted Yield':list(PredictedYield_test)})\n",
        "dfYield.to_excel(yieldFile, index=False)\n",
        "\n",
        "plotOriginalPredicted(OriginalYield_test, PredictedYield_test, plot, label='Predicted yield of {}'.format(mlaL))\n",
        "df_stats = performanceStatistics(OriginalYield_test, PredictedYield_test)\n",
        "print(mlaL, df_stats)\n",
        "\n",
        "df_GSCV = pd.DataFrame(GSCV.cv_results_)\n",
        "df_GSCV = df_GSCV.sort_values(by='rank_test_score', ascending=True)\n",
        "df_GSCV = df_GSCV.head(1)\n",
        "for i in list(df_GSCV.columns):\n",
        "       \n",
        "    df_stats[i] = df_GSCV[i].values.tolist()[0]\n",
        "            \n",
        "    df_stats = pd.DataFrame.from_dict(df_stats, orient='index').T\n",
        "    df_stats.to_excel(Stats, index=False)\n",
        "    \n",
        "    pi = inspection.permutation_importance(bestEstimator, X_test, y_test, n_jobs=-1, random_state=0).importances_mean\n",
        "    pi = [((i / pi.sum()) * 100) for i in pi]\n",
        "    dfImportances = pd.DataFrame(data=[pi], columns=xFeatures).round(2)\n",
        "    dfImportances.to_excel(Importances, index=False)"
      ],
      "metadata": {
        "id": "Gr336L25_5GY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "R2 of testing ANN using Univariate approach = 0.72, lets try Chi2 approach and compare.\n",
        "2- Chi-square Test\n",
        "\n",
        "The Chi-square test is used for categorical features in a dataset. We calculate Chi-square between each feature and the target and select the desired number of features with the best Chi-square scores. In order to correctly apply the chi-squared to test the relation between various features in the dataset and the target variable, the following conditions have to be met: the variables have to be categorical, sampled independently, and values should have an expected frequency greater than 5."
      ],
      "metadata": {
        "id": "yNJZ4j8dAUKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"D:/ICARDA/TT2/TestingByControl_1/Final merged file/Kheir_Codes and results for building ML to predict potato in Rwanda/Feature selection techniques_basedRFE/Cleaned1.csv\")\n",
        "from sklearn.feature_selection import SelectKBest, chi2 #Imprort the required library\n",
        "#remove minus and NAN values from dataframe\n",
        "df[df < 0] = 0\n",
        "df\n",
        "df = df.dropna()\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "print (df)"
      ],
      "metadata": {
        "id": "KHwxq7j6AM8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Specify X and y variables\n",
        "X = df.iloc[:,1:70]  #independent variables\n",
        "y = df.iloc[:,-1]    #target variable i.e price range\n",
        "X\n",
        "y=y.astype('int')\n",
        "y"
      ],
      "metadata": {
        "id": "AhW9W7bdBNHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert categorical data to int\n",
        "X_cat = X.astype(int)\n",
        "#20 features of the high ch2 will be selected\n",
        "chi2_features = SelectKBest(chi2,k=20)\n",
        "X_kbest_features = chi2_features.fit_transform(X_cat, y)\n",
        "\n",
        "#Reduced features\n",
        "print('Original feature number:',X_cat.shape[1])\n",
        "print('Reduced feature number:',X_kbest_features.shape[1])"
      ],
      "metadata": {
        "id": "Li-f2wL0BY2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_kbest_features.shape)\n",
        "print(X_kbest_features)\n",
        "X_kbest_features.astype('float')\n",
        "#Array to list to enalble us reading the feature values\n",
        "arr = np.array(X_kbest_features)\n",
        "list = arr.tolist()\n",
        "print(f'List: {list}')"
      ],
      "metadata": {
        "id": "VaneVyxzBajb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The selected 20 features from Chi2 approach are ['Cu030','K030','P030','Ptot030','albottom','catop','cabottom','fetop','ktop','kbottom','mgtop','mgbottom','ntotncstop','ntotncsbottom','DEM','TRI','Nrate','Prate','Krate']\n",
        "Using the same way used above with Univariate, it was found that the accuracy from Chi2 in testing Control= ANN {'R2': 0.65, 'RB': 4.75, 'MAE': 1.79, 'RMSE': 2.27, 'n': 100}. It looks like that there is an opportunity for improving feature selection if we compared both approaches with initial (without feature selection). Lets try feature permutation importance and compare its accuracy with Univariate and Chi2 approaches."
      ],
      "metadata": {
        "id": "uD-9SqLpBmE-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accurcy of ANN (the best estimator) using all features and without permutation importance, R2=0.56 (see upper cell at the begining of the code), lets drop the following features \n",
        "based on the PI, and see the accuracy under selected features ['ntotncsbottom', 'SOMbottom', 'FCbottom', 'octop', 'znbottom', 'ececfbottom', 'mgbottom']\n",
        "* ANN important features:59 (the selected features after RFE using permutation importance)"
      ],
      "metadata": {
        "id": "ELfem2k0B608"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#X and y features after dropping non-important features based on permutation importance from ANN\n",
        "xFeatures = ['B030','Cu030','K030','Mn030','N030','Na030','P030','Ptot030','altop','albottom','bdr','ctottop',\n",
        "'ctotbottom','cabottom','claytotpsatop','dbodtop','dbodbottom','ececftop',\n",
        "'fetop','febottom','ktop','kbottom','mgtop','ntotncstop','ocbottom','ptop','pbottom','phh2otop','phh2obottom',\n",
        "'stop','sbottom','sandtotpsatop','sandtotpsabottom','silttotpsatop','silttotpsabottom','wpg2top','wpg2bottom','zntop',\n",
        "'PWPtop','PWPbottom','FCtop','SWStop','SWSbottom','treat2','DEM','slope',\n",
        "'TPI','TRI','tr','di','nrd','tmean','tmin','tmax','Nrate','Prate','Krate'] \n",
        "\n",
        "xFeaturesML = ['B030','Cu030','K030','Mn030','N030','Na030','P030','Ptot030','altop','albottom','bdr','ctottop',\n",
        "'ctotbottom','cabottom','claytotpsatop','dbodtop','dbodbottom','ececftop',\n",
        "'fetop','febottom','ktop','kbottom','mgtop','ntotncstop','ocbottom','ptop','pbottom','phh2otop','phh2obottom',\n",
        "'stop','sbottom','sandtotpsatop','sandtotpsabottom','silttotpsatop','silttotpsabottom','wpg2top','wpg2bottom','zntop',\n",
        "'PWPtop','PWPbottom','FCtop','SWStop','SWSbottom','treat2','DEM','slope',\n",
        "'TPI','TRI','tr','di','nrd','tmean','tmin','tmax','Nrate','Prate','Krate'] \n",
        "\n",
        "yFeatur = 'yield'"
      ],
      "metadata": {
        "id": "mQC4GpTJCa8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Work space"
      ],
      "metadata": {
        "id": "TfFZDFjtC0RE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inFile = 'D:/ICARDA/TT2/TestingByControl_1/Final merged file/Kheir_Codes and results for building ML to predict potato in Rwanda/Feature selection techniques_basedRFE'\n",
        "outFolder = 'D:/ICARDA/TT2/TestingByControl_1/Final merged file/Kheir_Codes and results for building ML to predict potato in Rwanda/Feature selection techniques_basedRFE/ANNFPITraining'"
      ],
      "metadata": {
        "id": "rktRGqqQC1zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Default training (training by 75 % from the dataset and testing by 25 %)after dropping the non important features based on permuatation\n",
        "df = pd.read_csv('D:/ICARDA/TT2/TestingByControl_1/Final merged file/Kheir_Codes and results for building ML to predict potato in Rwanda/Feature selection techniques_basedRFE/Cleaned3.csv')\n",
        "df = df.dropna()\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "Xdf = df[xFeatures]\n",
        "ydf = df[[yFeatur]]\n",
        "\n",
        "X = df[xFeatures]\n",
        "y = df[[yFeatur]]\n",
        "X = (X - Xdf.mean()) / Xdf.std()\n",
        "y = (y - ydf.mean()) / ydf.std()\n",
        "df = pd.concat([X, y], axis = 1)\n",
        "df = df.dropna()\n",
        "X = df[xFeatures]\n",
        "y = df[[yFeatur]]\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25, random_state=0)\n",
        "\n",
        "n = len(xFeatures)\n",
        "MLA = hyperparameters(n)\n",
        "for idMLA, mlaL_mlaA in enumerate(MLA.items()):\n",
        "  mlaL, mlaA = mlaL_mlaA\n",
        "  estimator = mlaA[0]\n",
        "  param_grid = mlaA[1]\n",
        "  GSCV = model_selection.GridSearchCV(estimator, param_grid, scoring='r2', cv=5, refit=True, verbose=False, n_jobs=-1)\n",
        "  GSCV.fit(X_train, y_train)\n",
        "  bestEstimator = GSCV.best_estimator_ \n",
        "\n",
        "  OriginalYield_test  = y_test.values.flatten()\n",
        "  PredictedYield_test = bestEstimator.predict(X_test).flatten()\n",
        "  OriginalYield_test  = (float(ydf.std()) * OriginalYield_test) + float(ydf.mean())\n",
        "  PredictedYield_test = (float(ydf.std()) * PredictedYield_test) + float(ydf.mean())\n",
        "  \n",
        "  outFile     = os.path.join(outFolder, '{}_plot_DefaultML_{}.png'.format(mlaL, n))\n",
        "  Stats       = os.path.join(outFolder, '{}_Stats_DefaultML_{}.xlsx'.format(mlaL, n))\n",
        "  Importances = os.path.join(outFolder, '{}_Importances_DefaultML_{}.xlsx'.format(mlaL, n))\n",
        "\n",
        "  plotOriginalPredicted(OriginalYield_test, PredictedYield_test, outFile, label='Predicted yield of {}'.format(mlaL))\n",
        "  df_stats = performanceStatistics(OriginalYield_test, PredictedYield_test)\n",
        "  print(mlaL, df_stats)\n",
        "\n",
        "  df_GSCV = pd.DataFrame(GSCV.cv_results_)\n",
        "  df_GSCV = df_GSCV.sort_values(by='rank_test_score', ascending=True)\n",
        "  df_GSCV = df_GSCV.head(1)\n",
        "  for i in list(df_GSCV.columns):\n",
        "    df_stats[i] = df_GSCV[i].values.tolist()[0]\n",
        "  df_stats = pd.DataFrame.from_dict(df_stats, orient='index').T\n",
        "  df_stats.to_excel(Stats, index=False)\n",
        "  \n",
        "  pi = inspection.permutation_importance(bestEstimator, X_train, y_train, n_jobs=-1, random_state=0).importances_mean\n",
        "  pi = [((i / pi.sum()) * 100) for i in pi]\n",
        "  dfImportances = pd.DataFrame(data=[pi], columns=xFeatures).round(2)\n",
        "  dfImportances.to_excel(Importances, index=False)"
      ],
      "metadata": {
        "id": "9X2pyJiQC7D_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "R2 = 0.85, lets test by Control treatment\n"
      ],
      "metadata": {
        "id": "0s0rf78LDPaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##OutFolder2 another space of outputs of testing\n",
        "outFolder = 'D:/ICARDA/TT2/TestingByControl_1/Final merged file/Kheir_Codes and results for building ML to predict potato in Rwanda/Feature selection techniques_basedRFE/ANNFPITesting'\n",
        "#Test by b Control \n",
        "for i in [[xFeaturesML, 'ML'], [xFeaturesML, 'ML']]:\n",
        "\n",
        "  df = pd.read_csv('D:/ICARDA/TT2/TestingByControl_1/Final merged file/Kheir_Codes and results for building ML to predict potato in Rwanda/Feature selection techniques_basedRFE/Cleaned3.csv')\n",
        "  df = df.dropna()\n",
        "  df = df.reset_index(drop=True)\n",
        "\n",
        "  xFeatures = i[0]\n",
        "  group     = i[1]\n",
        "  \n",
        "  print(group)\n",
        "  Xdf = df[xFeatures]\n",
        "  ydf = df[[yFeatur]]\n",
        "\n",
        "  for col in df.columns:\n",
        "    if col not in ['texture_class_top','texture_class_bottom','treat']:\n",
        "      mean = np.mean(df[col].values)\n",
        "      std  = np.std(df[col].values)\n",
        "      if std != 0:\n",
        "        df[col] = df[col].apply(lambda x:(x-mean)/std)\n",
        "  \n",
        "  testtreat = ['Control'] # random.sample(list(df.treat.unique()), 1)\n",
        "  print(testtreat)\n",
        "\n",
        " \n",
        "  test  = df[df['treat'].isin(testtreat)]\n",
        "\n",
        " \n",
        "  X_test  = test[xFeatures]\n",
        "  y_test  = test[[yFeatur]]\n",
        "#Test by treat(Control) and best estimator\n",
        "n = len(xFeatures)\n",
        "  #MLA = hyperparameters(n)\n",
        "  #for idMLA, mlaL_mlaA in enumerate(MLA.items()):\n",
        "    #mlaL, mlaA = mlaL_mlaA\n",
        "    #estimator = mlaA[0]\n",
        "    #param_grid = mlaA[1]\n",
        "    #GSCV = model_selection.GridSearchCV(estimator, param_grid, scoring='r2', cv=5, refit=True, verbose=False, n_jobs=-1)\n",
        "GSCV.predict(X_test)\n",
        "bestEstimator = GSCV.best_estimator_ \n",
        "\n",
        "OriginalYield_test  = y_test.values.flatten()\n",
        "PredictedYield_test = bestEstimator.predict(X_test).flatten()\n",
        "OriginalYield_test  = (float(ydf.std()) * OriginalYield_test) + float(ydf.mean())\n",
        "PredictedYield_test = (float(ydf.std()) * PredictedYield_test) + float(ydf.mean())\n",
        "\n",
        "yieldFile   = os.path.join(outFolder, '{}_yield_{}_{}.xlsx'.format(mlaL, group, n))\n",
        "plot        = os.path.join(outFolder, '{}_plot_{}_{}.png'.format(mlaL, group, n))\n",
        "Stats       = os.path.join(outFolder, '{}_Stats_{}_{}.xlsx'.format(mlaL, group, n))\n",
        "Importances = os.path.join(outFolder, '{}_Importances_{}_{}.xlsx'.format(mlaL, group, n))\n",
        "\n",
        "dfYield = pd.DataFrame({'Original Yield':list(OriginalYield_test), 'Predicted Yield':list(PredictedYield_test)})\n",
        "dfYield.to_excel(yieldFile, index=False)\n",
        "\n",
        "plotOriginalPredicted(OriginalYield_test, PredictedYield_test, plot, label='Predicted yield of {}'.format(mlaL))\n",
        "df_stats = performanceStatistics(OriginalYield_test, PredictedYield_test)\n",
        "print(mlaL, df_stats)\n",
        "\n",
        "df_GSCV = pd.DataFrame(GSCV.cv_results_)\n",
        "df_GSCV = df_GSCV.sort_values(by='rank_test_score', ascending=True)\n",
        "df_GSCV = df_GSCV.head(1)\n",
        "for i in list(df_GSCV.columns):\n",
        "       \n",
        "    df_stats[i] = df_GSCV[i].values.tolist()[0]\n",
        "            \n",
        "    df_stats = pd.DataFrame.from_dict(df_stats, orient='index').T\n",
        "    df_stats.to_excel(Stats, index=False)\n",
        "    \n",
        "    pi = inspection.permutation_importance(bestEstimator, X_test, y_test, n_jobs=-1, random_state=0).importances_mean\n",
        "    pi = [((i / pi.sum()) * 100) for i in pi]\n",
        "    dfImportances = pd.DataFrame(data=[pi], columns=xFeatures).round(2)\n",
        "    dfImportances.to_excel(Importances, index=False)"
      ],
      "metadata": {
        "id": "46_gDTEoDWoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, Permutation importances based ANN, showed the best accuracy of testing by Control (R2=0.78), compared with Univariate (R2=0.72), and Chi2 (R2=0.65). Therefore, based on the current dataset and region conditions, the best estimator of yield is ANN and best approach of feature selection is the permutation importance from ANN. Lets use this script in the response function of spatial N,P, and K."
      ],
      "metadata": {
        "id": "RBzvX5b1DjYR"
      }
    }
  ]
}